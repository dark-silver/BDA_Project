---
title: "BDA Project"
author: "Anonymous"
output:
  bookdown::pdf_document2
toc_depth: 1
urlcolor: blue
bibliography: Bibliography.bib
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
# This chunk sets echo = TRUE as default, that is print all code.
# knitr::opts_chunk$set can be used to set other notebook generation options, too.
# include=FALSE inside curly brackets makes this block not be included in the pdf.
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
#install.packages("readxl")
#nstall.packages("DescTools")
library(readxl)
library(tidyr)
#library(DescTools)
library(plyr)
library(ggplot2)
library(reshape2)
library(rstan) 
#library(cmdstanr)
rstan_options(auto_write = TRUE)
options(mc.cores = 1)
# library(loo)
library(bayesplot)
library(posterior)
library(knitr)
#install.packages('cowplot')
library(cowplot)
```

# Introduction

# Data

The data used in this project comes from the "Advancing Sustainable Materials Management: Facts and Figures Report" of the United States Environmental Protection Agency (@epa_2022). The data is available for download through this link: <https://edg.epa.gov/data/PUBLIC/OLEM/Materials_Municipal_Waste_Stream_1960_2018.xlsx>.

The data is in Excel format and contains waste management data from the US for the years 1960-2018. This document contains a large amount of sheets, but for this project, only the "Materials generated", "Materials recycled", "Material combusted", and "Materials landfilled" sheets were used, as they are the only ones with clear data for textiles.

The data from 1960 to 2000 is given every ten years, then there is data for 2005, and the data from 2010-2018 is given yearly. Due to this inconsistency in time intervals of data, only part of the years are used to fit the models. The years 2010-2018 were chosen, as they have the most regular data and they are the most recent. The rest of the data is used to help in estimating prior parameters.

Also, the sheet "Materials generated" is not used in the models as it isn't interesting to the goal of the project, but it is used as an aid in prior parameter determination.

## The final data {#the-final-data}

As the data is scattered in multiple sheets of an Excel file, the specific textile-related data had to be extracted from each sheet, combined, and the row and column names cleaned up a bit in order to acquire a final dataset. This results in a dataframe with categories "Materials generated", "Materials recycled", "Material combusted", and "Materials landfilled".

The year is also adjusted such that the year 1950 becomes the new year 0 (1950 is subtracted from each year). This is done so that the significantly long period of time before data collection where values for each category might have been 0 doesn't affect the final models. For example, the first landfill in the US was established in 1937 (@zylberberg_2019), so there would have been 1937 years of 0 values in the "Materials landfilled" category, which would have a significant impact on the (slope of the linear) models. Hence, a new year 0 was chosen in which all categories of waste management had already been implemented/invented.

```{r, results='hide', include=FALSE}
setwd(getwd())

multiplesheets <- function(fname) {
  
  # getting info about all excel sheets
  sheets <- readxl::excel_sheets(fname)
  tibble <- lapply(sheets, function(x) readxl::read_excel(fname, sheet = x))
  data_frame <- lapply(tibble, as.data.frame)
  
  # assigning names to data frames
  names(data_frame) <- sheets
  
  init_df <<- data_frame
}

# specifying the path name
path <- paste(getwd(), "/Materials_Municipal_Waste_Stream_1960_2018.xlsx", sep="")
multiplesheets(path)

# Dataframe for storing the textile data
all_data <- data.frame(matrix(ncol = 0, nrow = 15))

# Add the data to the dataframe
for (name in names(init_df)) {
  df <- init_df[name][[1]]
  no_na <- drop_na(df)
  row_df <- data.frame(no_na[,-1], row.names=no_na[,1])
  new_df <- row_df[grep("Textiles", rownames(row_df)), ]
  t_df <- t(new_df)
  if (ncol(t_df) > 0) {
    colnames(t_df) <- c(name)
    all_data <<- cbind(all_data, t_df)
  }
}

# Clean up row names
rownames(all_data) <-gsub("X","",as.character(rownames(all_data)))

all_data['year'] <- as.numeric(as.character(rownames(all_data)))

rownames(all_data) <- NULL

plot_his_frame <- all_data[1:6,]
plot_frame <- all_data[7:15,]

#Adjust 1950 to be year 0:
all_data['original year'] <- all_data['year']
all_data['year'] <- all_data['year']-1950
```

The final dataframe can be seen in Table \@ref(tab:final-df).

```{r final-df, echo=FALSE}
kable(all_data, caption='Final dataframe')
```

The final data is visualised in Figure \@ref(fig:data-his-plot) for years 1960-2005, and in Figure \@ref(fig:data-plot) for years 2010-2018.

```{r data-his-plot, echo=FALSE, fig.cap="Data for years 1960-2005.", fig.height=2, fig.width=6}
par(mfrow=c(1,3))
plot(x=plot_his_frame[['year']], y=plot_his_frame[['Materials recycled']], type='l', xlab='year', ylab='Materials recycled (tons)')
plot(x=plot_his_frame[['year']], y=plot_his_frame[['Material combusted']], type='l', xlab='year', ylab='Materials combusted (tons)')
plot(x=plot_his_frame[['year']], y=plot_his_frame[['Materials landfilled']], type='l', xlab='year', ylab='Materials landfilled (tons)')

```

```{r data-plot, echo=FALSE, fig.cap="Data for years 2010-2018.", fig.height=2, fig.width=6}
par(mfrow=c(1,3))
plot(x=plot_frame[['year']], y=plot_frame[['Materials recycled']], type='l', xlab='year', ylab='Materials recycled (tons)')
plot(x=plot_frame[['year']], y=plot_frame[['Material combusted']], type='l', xlab='year', ylab='Materials combusted (tons)')
plot(x=plot_frame[['year']], y=plot_frame[['Materials landfilled']], type='l', xlab='year', ylab='Materials landfilled (tons)')

```

As can be seen from the figures, the data could likely be approximated decently well with a linear model.

# The models

Since the data used for modeling in this project is split into three categories (materials recycled, materials combusted, and materials landfilled), and their differences are an aspect of interest, a model is needed which differentiates between them. Two good options for this are the separate model and the hierarchical model. Hence, in this project both of these models are implemented and compared.

## Separate model

The separate model assumes that each category is independent from other categories, and essentially gives each category its own Bayesian model. Hence, it considers the categories to have no relation between each other.

## Hierarchical model

The hierarchical model considers each category as being distinct from each other, but still similar enough that the measurements from one category can influence the predictions for another. To accommodate for this, the hierarchical model provides separate models for each category, however the models share the same variance and the means of the models share the same prior distribution constructed with hyperparameters. Hence, the hierarchical model considers the categories to be distinct, but still aknowledges some relation between them.

## Linear Gaussian model

As was shown in the "Data" section, the data used to fit the models is approximately linear, with the explanatory variable being the year. Hence, both the separate and hierarchical models are implemented as linear gaussian models in this project. This means the parameters of interest are the intercepts ($\alpha$) and slopes ($\beta$) of the lines describing the data. The likelihood for this model is as follows:

$$
y \sim N(\alpha+\beta*x, \sigma).
$$

# Priors

```{r include=FALSE}
# Separate model:
# Estimate intercept means:
# recycled_intercept <- 680000*0.001 US population in year 0  times 1kg (0.001 tonnes) 
# for clothing waste in a year per person
# combusted_intercept <- 0
# landfilled_intercept <- 0

#Estimate slope means:
summary_2 <- summary(lm(`Materials recycled` ~ year, data=all_data[(1:6),]))
recycled_slope <- summary_2$coefficients[2,1]

summary_3 <- summary(lm(`Material combusted` ~ year, data=all_data[(1:6),]))
combusted_slope <- summary_3$coefficients[2,1]

summary_4 <- summary(lm(`Materials landfilled` ~ year, data=all_data[(1:6),]))
landfilled_slope <- summary_4$coefficients[2,1]

beta_mean = c(recycled_slope, combusted_slope, landfilled_slope)

sd_mean = c(sd(all_data[1:6,'Materials recycled']), 
            sd(all_data[1:6,'Material combusted']), 
            sd(all_data[1:6,'Materials landfilled']))
sd_mean_old <- sd_mean

# Calculate the factor by how much the standard deviation of generated textiles 
# has shrunk after between 1960-2005 and 2010-2018
factor <- sd(all_data[1:6, 'Materials generated'])/sd(all_data[7:15, 'Materials generated'])
# Divide sd's by the factor to make them more realistic
sd_mean <- sd_mean/factor

# Intercept means for 1950 0:
recycled_intercept <- max(summary_2$coefficients[1,1], 0)
combusted_intercept <- max(summary_3$coefficients[1,1], 0)
landfilled_intercept <- max(summary_4$coefficients[1,1], 0)
alpha_mean = c(recycled_intercept, combusted_intercept, landfilled_intercept)

# Hierarchical model:
hierarchical_intercept_mean <- mean(alpha_mean)
hierarchical_slope_mean <- mean(beta_mean)
hierarchical_sd_mean_old <- mean(sd_mean_old)
hierarchical_sd_mean <- mean(sd_mean)

```

## Initial priors

Initial priors refer to the first priors which were tested.

### Separate model

The priors for $\alpha$, $\beta$, and $\sigma$ are normal distributions, because those are convenient considering the likelihood is also a normal distribution. Since the separate model is in question, separate priors are needed for each category ("Materials recycled", "Material combusted", and "Materials landfilled").

To get the mean parameter of the $\alpha$ and $\beta$ priors ($\mu$ in $N(\mu,\sigma)$), a linear model was fit for the 1960-2005 data of each category. Then, the means ($\mu$) of the $\alpha$ priors for each category were set to be the maximum of 0 and the intercept extracted from the corresponding linear model (the intercept shouldn't be negative as waste management can't take on a negative value). Then, the means ($\mu$) of the $\beta$ priors for each category were set to be the slope extracted from the corresponding model.

The standard deviation parameters of the $\alpha$ and $\beta$ priors ($\sigma$ in $N(\mu,\sigma)$), were set to be 100 and 1000, respectively, for each category. This was chosen quite arbitrarily to represent the prior being weakly informative (the sd for $\beta$ is bigger than that of $\alpha$ because the slopes had much larger values than intercepts).

The means for the $\sigma$ priors were set to be the standard deviations of the data for the corresponding category from years 1960-2005. The standard deviations for the $\sigma$ priors were set to be 1000 to represent weak informativeness.

This resulted in the priors shown in Table \@ref(tab:s-p-i-tab).

```{r s-p-i-tab, echo=FALSE}
alphas <- c()
betas <- c()
sigmas <- c()
for (i in alpha_mean) {
  alphas <- append(alphas, paste('N(', round(i, digits=0), ',', 100, ')', sep=""))
}
for (i in beta_mean) {
  betas <- append(betas, paste('N(', round(i, digits=0), ',', 1000, ')', sep=""))
}
for (i in sd_mean_old) {
  sigmas <- append(sigmas, paste('N(', round(i, digits=0), ',', 1000, ')', sep=""))
}
s_p_init_matr = cbind(alphas, betas, sigmas)
colnames(s_p_init_matr) = c('Alpha', 'Beta', 'Sigma')
rownames(s_p_init_matr) = c('Materials recycled', 'Materials combusted', 'Materials landfilled')
kable(s_p_init_matr, caption="Separate model intial priors (values rounded to 0 digits).")
```

### Hierarchical model

For the hierarchical model, priors were needed for the means ($\mu$ in $N(\mu,\sigma)$) and standard deviations ($\sigma$ in $N(\mu,\sigma)$) for both $\alpha$ and $\beta$, and then for the common standard deviation $\sigma$ (5 priors in total: $\alpha$-mean, $\alpha$-sd, $\beta$-mean, $\beta$-sd, $\sigma$).

The prior for the $\alpha$-mean was set to have the average of the intercept estimates from the three categories as its mean, and 10 as it's standard deviation because we were quite confident in the mean value of the $\alpha$-mean (10 is a fairly small number so it made the prior informative).

The prior for the $\alpha$-sd was set to have the sum of the standard deviations of the $\alpha$ priors from the separate model as its mean, and 100 as it's standard deviation to make the prior weakly informative.

The prior for the $\beta$-mean was set to have the average of the slope estimates from the three categories as its mean, and 100 as it's standard deviation because we were again quite confident in the mean value of the $\beta$-mean (100 is quite a small value compared to the average of the slopes, so this made the prior informative).

The prior for the $\beta$-sd was set to have the sum of the standard deviations of the $\beta$ priors from the separate model as its mean, and 1000 as it's standard deviation to make the prior weakly informative (again, the standard deviation for $\beta$-sd prior is larger than for $\alpha$-sd prior because the values for the slopes are much larger).

The prior for $\sigma$ was set to have the mean of the standard deviations of the 1960-2005 data of each category as its mean, and 1000 as its standard deviation to make it weakly informative.

The resulting hyperpriors (and sigma prior) can be seen in Table \@ref(tab:h-p-i-tab), and their usage can be seen in the following formula:

$$
\begin{aligned}
\alpha_i \sim N(\alpha\text{-}mean, \alpha\text{-}sd)\\
\beta_i \sim N(\beta\text{-}mean, \beta\text{-}sd)\\
y_i \sim N(\alpha_i + \beta_i*x, \sigma)
\end{aligned}
$$

```{r h-p-i-tab, echo=FALSE}
h_p_init_matr <- c(paste('N(', round(hierarchical_intercept_mean, digits=0), ',', 10, ')', sep=""),
                   paste('N(', 300, ',', 100, ')', sep=""),
                   paste('N(', round(hierarchical_slope_mean, digits=0), ',', 100, ')', sep=""),
                   paste('N(', 3000, ',', 1000, ')', sep=""),
                   paste('N(', round(hierarchical_sd_mean_old, digits=0), ',', 1000, ')', sep=""))
h_p_init_matr <- rbind(h_p_init_matr)
colnames(h_p_init_matr) = c('Alpha-mean', 'Alpha-sd', 'Beta-mean', 'Beta-sd', 'Sigma')
rownames(h_p_init_matr) <- c('Prior')
kable(h_p_init_matr, caption="Initial hyperpriors (and sigma prior) for the hierarchical model (values rounded to 0 digits).")
```

## Final priors

Final priors refer to the priors obtained after all necessary modifications from analysis (the changes are also described throughout the analysis).

### Separate model

For the separate model, it was realised during analysis that the standard deviations of the $\alpha$ and $\beta$ priors were too small compared to the values in the data, and the priors weren't actually weakly informative. Hence, the standard deviation for the $\alpha$ prior was increased to 1000, and the standard deviation for the $\beta$ prior was increased to 10000.

In addition, the mean of the $\sigma$ prior was modified. The standard deviations of years 1960-2005 and years 2010-2018 were compared for the "Materials generated" data (which wasn't included in the model). The standard deviation is almost three times larger for 1969-2005 than for 2010-2018. Hence, the standard deviations of the $\sigma$ priors were divided by this coefficient (sd-previous-years/sd-recent-years). As "Materials generated" is the sum of the other categories, it is sensible to assume that the trends in the former would be similar to those in the latter, so dividing by the coefficient is sensible as well.

These changes resulted in the priors show in Table \@ref(tab:s-p-f-tab).

```{r s-p-f-tab, echo=FALSE}
alphas <- c()
betas <- c()
sigmas <- c()
for (i in alpha_mean) {
  alphas <- append(alphas, paste('N(', round(i, digits=0), ',', 1000, ')', sep=""))
}
for (i in beta_mean) {
  betas <- append(betas, paste('N(', round(i, digits=0), ',', 10000, ')', sep=""))
}
for (i in sd_mean) {
  sigmas <- append(sigmas, paste('N(', round(i, digits=0), ',', 1000, ')', sep=""))
}
s_p_final_matr = cbind(alphas, betas, sigmas)
colnames(s_p_final_matr) = c('Alpha', 'Beta', 'Sigma')
rownames(s_p_final_matr) = c('Materials recycled', 'Materials combusted', 'Materials landfilled')
kable(s_p_final_matr, caption="Separate model final priors (values rounded to 0 digits).")
```

### Hierarchical model

For the hierarchical model, the standard deviations of the $\alpha$-mean and $\beta$-mean priors were increased, because after some thought it was concluded that the standard deviations were too small compared to the scale of the data. The standard deviations were increased to 100 and 1000, respectively.

The parameters for the $\alpha$-sd and $\beta$-sd priors were also recalculated according to the new separate model priors. Also, instead of summing the standard deviations of the separate model $\alpha$ and $\beta$ priors, the average was taken to acquire the prior mean. This made more sense as the $\alpha$-mean and $\beta$-mean prior means are also calculated as an average. The standard deviations of the $\alpha$-sd and $\beta$-sd priors were changed accordingly to 200 and 2000, respectively, to represent weak informativity.

Similarly to the separate model, the mean of the $\sigma$ prior was divided by the factor (sd-previous-years/sd-recent-years). In addition, the standard deviation of the $\sigma$ prior had to be increased quite drastically, to 100000, for the posterior predictive checks to look good. This value still made sense with the scale of the data as the and the differences of standard deviations between the categories were larger than this value.

The resulting hyperpriors (and sigma prior) can be seen in Table \@ref(tab:h-p-f-tab).

```{r h-p-f-tab, echo=FALSE}
h_p_init_matr <- c(paste('N(', round(hierarchical_intercept_mean, digits=0), ',', 100, ')', sep=""),
                   paste('N(', 1000, ',', 200, ')', sep=""),
                   paste('N(', round(hierarchical_slope_mean, digits=0), ',', 1000, ')', sep=""),
                   paste('N(', 10000, ',', 2000, ')', sep=""),
                   paste('N(', round(hierarchical_sd_mean, digits=0), ',', 100000, ')', sep=""))
h_p_final_matr <- rbind(h_p_init_matr)
colnames(h_p_final_matr) = c('Alpha-mean', 'Alpha-sd', 'Beta-mean', 'Beta-sd', 'Sigma')
rownames(h_p_final_matr) <- c('Prior')
kable(h_p_final_matr, caption="Final hyperpriors (and sigma prior) for the hierarchical model (values rounded to 0 digits).")
```

# Stan code

## Separate

Stan code with final priors:

```{r echo=FALSE}
writeLines(readLines("Separate_project.stan"))
```

## Hierarchical

Stan code with final priors:

```{r echo=FALSE}
writeLines(readLines("Hierarchical_project.stan"))
```

# Fitting the models

```{r include=FALSE, results='hide'}
# Prepare data
data_hierarchical = list(
  N = nrow(all_data[7:15,]),
  J = ncol(all_data[,2:4]),
  x = all_data[7:15,'year'],
  y = all_data[7:15, 2:4],
  xpred = (2019-1950),
  sd_mean = hierarchical_sd_mean,
  slope_mean = hierarchical_slope_mean,
  intercept_mean = hierarchical_intercept_mean
)

data_separate = list(
  N = nrow(all_data[7:15,]),
  J = ncol(all_data[,2:4]),
  x = all_data[7:15,'year'],
  y = all_data[7:15, 2:4],
  xpred = (2019-1950),
  alpha_mean = alpha_mean,
  beta_mean = beta_mean,
  sds = sd_mean
)

```

Both models were fit with the default parameters for stan(), meaning 4 chains with 2000 iterations and 1000 warmup, and random initial points. This can be seen in the following code snippet:

```{r results='hide'}
fit_hierarchical <- stan(file = "Hierarchical_project.stan", data = data_hierarchical) 
fit_separate <- stan(file = "Separate_project.stan", data = data_separate)
```

# Convergence diagnostics

## R-hat and ESS

### Hierarchical

```{r hier-rhat, echo=FALSE, results='asis'}
hier_matr = matrix(data=NA, nrow=6, ncol=3)
colnames(hier_matr) = c('Rhat', 'bulk-ESS', 'tail-ESS')
rownames(hier_matr) = c('alpha[1]', 'beta[1]', 'alpha[2]', 'beta[2]', 'alpha[3]', 'beta[3]')
for (i in 1:3) {
  alpha <- paste('alpha[', as.character(i), ']', sep="")
  beta <- paste('beta[', as.character(i), ']', sep="")
  alpha_mat <- extract_variable_matrix(fit_hierarchical, alpha)
  beta_mat <- extract_variable_matrix(fit_hierarchical, beta)
  rhat_alpha <- posterior::rhat(alpha_mat)
  rhat_beta <- posterior::rhat(beta_mat)
  b_ess_alpha <- posterior::ess_bulk(alpha_mat)
  b_ess_beta <- posterior::ess_bulk(beta_mat)
  t_ess_alpha <- posterior::ess_tail(alpha_mat)
  t_ess_beta <- posterior::ess_tail(beta_mat)
  hier_matr[i+(i-1), 1] <- rhat_alpha
  hier_matr[i+(i-1)+1, 1] <- rhat_beta
  hier_matr[i+(i-1), 2] <- b_ess_alpha
  hier_matr[i+(i-1)+1, 2] <- b_ess_beta
  hier_matr[i+(i-1), 3] <- t_ess_alpha
  hier_matr[i+(i-1)+1, 3] <- t_ess_beta
}
kable(hier_matr, caption='Hierarchical model Rhat and ESS.')
```

The hierarchical model had good Rhat and ESS from the start. For the final priors the Rhat and ESS are still very good. This means that all Rhat values are below 1.05 and all ESS values are larger than 100 which indicate convergence. Table \@ref(tab:hier-rhat) shows the Rhat and ESS values for the hierarchical model with final priors.

### Separate

```{r sep-rhat, echo=FALSE, results='asis'}
sep_matr = matrix(data=NA, nrow=6, ncol=3)
colnames(sep_matr) = c('Rhat', 'bulk-ESS', 'tail-ESS')
rownames(sep_matr) = c('alpha[1]', 'beta[1]', 'alpha[2]', 'beta[2]', 'alpha[3]', 'beta[3]')
for (i in 1:3) {
  alpha <- paste('alpha[', as.character(i), ']', sep="")
  beta <- paste('beta[', as.character(i), ']', sep="")
  alpha_mat <- extract_variable_matrix(fit_separate, alpha)
  beta_mat <- extract_variable_matrix(fit_separate, beta)
  rhat_alpha <- posterior::rhat(alpha_mat)
  rhat_beta <- posterior::rhat(beta_mat)
  b_ess_alpha <- posterior::ess_bulk(alpha_mat)
  b_ess_beta <- posterior::ess_bulk(beta_mat)
  t_ess_alpha <- posterior::ess_tail(alpha_mat)
  t_ess_beta <- posterior::ess_tail(beta_mat)
  sep_matr[i+(i-1), 1] <- rhat_alpha
  sep_matr[i+(i-1)+1, 1] <- rhat_beta
  sep_matr[i+(i-1), 2] <- b_ess_alpha
  sep_matr[i+(i-1)+1, 2] <- b_ess_beta
  sep_matr[i+(i-1), 3] <- t_ess_alpha
  sep_matr[i+(i-1)+1, 3] <- t_ess_beta
}
kable(sep_matr, caption='Separate model Rhat and ESS.')
```

For the initial priors, all the separate model's Rhat values were greater than 1.05 and most ESS values were smaller than 100, meaning the chains did not converge. Initially this was fixed through simplifying the model by removing the sigma prior entirely. The sigma prior was later added back with different parameters (final sigma prior in the section on priors), and the Rhat and ESS values for the final model can be seen in Table \@ref(tab:sep-rhat). As can be seen from the table, all Rhat values are smaller than 1.05, and all ESS values are larger than 100, meaning the final separate model has converged.

## Hamiltonian Monte Carlo (HMC) diagnostics

### Hierarchical

HMC diagnostics for the final hierarchical model:

```{r}
check_hmc_diagnostics(fit_hierarchical)
```

The hierarchical model had a few divergences in the initial model. Changing adapt_delta didn't seem to do much, so instead priors were modified. At this point, a mistake in the alpha_sigma and beta_sigma priors was noticed. In the separate model, the standard deviation parameters for alpha and beta were too small compared to the values in the data, meaning the priors weren't really weakly informative. The parameters of the alpha_sigma and beta_sigma priors in the hierarchical model are calculated based on these standard deviations in the separate model. This calculation was also nonsensical, summing all the standard deviations instead of averaging them. Both of these issues were addressed, after which there were no more divergences in the hierarchical model, as can be seen above.

Note: More details about the prior modification can be found in the Priors section.

### Separate

HMC diagnostics for the final separate model:

```{r}
check_hmc_diagnostics(fit_separate)
```

The separate model had no divergences or other issues with HMC for either the initial priors or the final priors.

# Posterior predictive checks

## Separate

Figures \@ref(fig:cs-plot), \@ref(fig:rs-plot), and \@ref(fig:ls-plot) show posterior predictive checks for the final separate model. The density plots suggest that the model is an okay fit as the replicate draws seem somewhat similar to the true data, but the histograms don't look quite as good. However, considering the dataset is very small, this extent of fit is satisfactory for this project.

For the initial model without a sigma prior, the fit was worse. The distributions were clearly too wide, so a sigma prior was added (the final sigma prior is discussed in the Priors section). This made the fit much better.

```{r cs-plot, fig.height=2, fig.width=7, fig.cap="Posterior predictive check for materials combusted with the separate model.", echo=FALSE, warning=FALSE}
# Materials combusted 2019 prediction
s_yrep_c <- as.matrix(fit_separate, pars = "yrep_c")
s_yrep_r <- as.matrix(fit_separate, pars = "yrep_r")
s_yrep_l <- as.matrix(fit_separate, pars = "yrep_l")
h_yrep_c <- as.matrix(fit_hierarchical, pars = "yrep_c")
h_yrep_r <- as.matrix(fit_hierarchical, pars = "yrep_r")
h_yrep_l <- as.matrix(fit_hierarchical, pars = "yrep_l")
y_c <- all_data[['Material combusted']]
y_r <- all_data[['Materials recycled']]
y_l <- all_data[['Materials landfilled']]
p_dens <- ppc_dens_overlay(y_c[7:15], yrep = s_yrep_c[1:50,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
p_hist <- ppc_hist(y_c[7:15], yrep = s_yrep_c[1:8,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
plot_grid(p_dens, p_hist)
```

```{r rs-plot, fig.height=2, fig.width=7, fig.cap="Posterior predictive check for materials recycled with the separate model.", echo=FALSE, warning=FALSE}
p_dens <- ppc_dens_overlay(y_r[7:15], yrep = s_yrep_r[1:50,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
p_hist <- ppc_hist(y_r[7:15], yrep = s_yrep_r[1:8,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
plot_grid(p_dens, p_hist)
```

```{r ls-plot, fig.height=2, fig.width=7, fig.cap="Posterior predictive check for materials landfilled with the separate model.", echo=FALSE, warning=FALSE}
p_dens <- ppc_dens_overlay(y_l[7:15], yrep = s_yrep_l[1:50,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
p_hist <- ppc_hist(y_l[7:15], yrep = s_yrep_l[1:8,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
plot_grid(p_dens, p_hist)
```

## Hierarchical

Figures \@ref(fig:ch-plot), \@ref(fig:rh-plot), and \@ref(fig:lh-plot) show posterior predictive checks for the final hierarchical model. The density plots suggest that the model is a somewhat decent (but worse than the separate model) fit as the replicate draws seem somewhat similar to the true data, but the histograms don't look quite as good. However, considering the dataset is very small again, this extent of fit is satisfactory for this project.

For the initial model without a sigma prior, the fit was much worse. The distributions were clearly too wide, so a sigma prior was added (the final sigma prior is discussed in the Priors section). This made the fit much better.

```{r ch-plot, fig.height=2, fig.width=7, fig.cap="Posterior predictive check for materials combusted with the hierarchical model.", echo=FALSE, warning=FALSE}
p_dens <- ppc_dens_overlay(y_c[7:15], yrep = h_yrep_c[1:50,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
p_hist <- ppc_hist(y_c[7:15], yrep = h_yrep_c[1:8,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
plot_grid(p_dens, p_hist)
```

```{r rh-plot, fig.height=2, fig.width=7, fig.cap="Posterior predictive check for materials recycled with the hierarchical model.", echo=FALSE, warning=FALSE}
p_dens <- ppc_dens_overlay(y_r[7:15], yrep = h_yrep_r[1:50,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
p_hist <- ppc_hist(y_r[7:15], yrep = h_yrep_r[1:8,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
plot_grid(p_dens, p_hist)
```

```{r lh-plot, fig.height=2, fig.width=7, fig.cap="Posterior predictive check for materials landfilled with the hierarchical model.", echo=FALSE, warning=FALSE}
p_dens <- ppc_dens_overlay(y_l[7:15], yrep = h_yrep_l[1:50,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
p_hist <- ppc_hist(y_l[7:15], yrep = h_yrep_l[1:8,]) +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
plot_grid(p_dens, p_hist)
```

# Predictive performance assessment {#predictive-performance-assessment}

To assess performance, we can fit the models excluding the year 2018, instead adding that year as the predicted year. We can then compare the true values for 2018 to the predicted values.

```{r include=FALSE}
# Prepare data
performance_data_hierarchical = list(
  N = nrow(all_data[7:14,]),
  J = ncol(all_data[,2:4]),
  x = all_data[7:14,'year'],
  y = all_data[7:14, 2:4],
  xpred = (2018-1950),
  sd_mean = hierarchical_sd_mean,
  slope_mean = hierarchical_slope_mean,
  intercept_mean = hierarchical_intercept_mean
)

performance_data_separate = list(
  N = nrow(all_data[7:14,]),
  J = ncol(all_data[,2:4]),
  x = all_data[7:14,'year'],
  y = all_data[7:14, 2:4],
  xpred = (2018-1950),
  alpha_mean = alpha_mean,
  beta_mean = beta_mean,
  sds = sd_mean
)

# Removed sigma priors and modified sd priors because it wasn't converging
fit_hierarchical_perf <- stan(file = "Hierarchical_project.stan", 
			      data = performance_data_hierarchical) 
fit_separate_perf <- stan(file = "Separate_project.stan", data = performance_data_separate)
```

The results for this analysis can be seen in Table \@ref(tab:perf-asses-h) for the hierarchical model, and Table \@ref(tab:perf-asses-s) for the separate model.

```{r perf-asses-h, echo=FALSE}
ypred_r_h <- extract(fit_hierarchical_perf, 'ypred_r')
ypred_c_h <- extract(fit_hierarchical_perf, 'ypred_c')
ypred_l_h <- extract(fit_hierarchical_perf, 'ypred_l')
pred_h <- c(ypred_r_h, ypred_c_h, ypred_l_h)
ypred_r_s <- extract(fit_separate_perf, 'ypred_r')
ypred_c_s <- extract(fit_separate_perf, 'ypred_c')
ypred_l_s <- extract(fit_separate_perf, 'ypred_l')
pred_s <- c(ypred_r_s, ypred_c_s, ypred_l_s)

real_vals <- all_data[all_data$year==(2018 - 1950),]
reals <- c(real_vals[['Materials recycled']], real_vals[['Material combusted']], 
           real_vals[['Materials landfilled']])

hier_pred = matrix(data=NA, nrow=3, ncol=3)
colnames(hier_pred) = c('Materials recycled', 'Materials combusted', 'Materials landfilled')
rownames(hier_pred) = c('Real', 'Predicted mean', 'Predicted sd')
for (i in 1:3) {
  hier_pred[1,i] <- reals[[i]]
  hier_pred[2,i] <- mean(pred_h[[i]])
  hier_pred[3,i] <- sd(pred_h[[i]])
}
kable(hier_pred, caption='Hierarchical model predictive performance assessment.')
```

```{r perf-asses-s, echo=FALSE}
sep_pred = matrix(data=NA, nrow=3, ncol=3)
colnames(sep_pred) = c('Materials recycled', 'Materials combusted', 'Materials landfilled')
rownames(sep_pred) = c('Real', 'Predicted mean', 'Predicted sd')
for (i in 1:3) {
  sep_pred[1,i] <- reals[[i]]
  sep_pred[2,i] <- mean(pred_s[[i]])
  sep_pred[3,i] <- sd(pred_s[[i]])
}
kable(sep_pred, caption='Separate model predictive performance assessment.')
```

Both models seem to perform somewhat well and quite similarly. The means of the predictions are relatively close to the real values, though standard deviations are quite large. Although the values aren't incredibly accurate, they could be used as a rough measure for guidance in waste management planning for upcoming years. They could also be used as a rough measure of how much waste production needs to be reduced to stay at the same or a lower level as the previous year.

# Sensitivity analysis

Two priors were tested for the sensitivity analysis in addition to the final priors described in the Priors section. These priors are the uniform prior (used for every parameter), and the combination of N(10000,100) for mean parameters and N(0,50000) for standard deviation parameters which was called the "Second" prior in this analysis.

Clarification of the Second prior for separate model:

```{r echo=FALSE}
writeLines(readLines('2ndprior_separate.stan')[18:22])
```

Clarification of the Second prior for hierarchical model:

```{r echo=FALSE}
writeLines(readLines('2ndprior_hierarchical.stan')[22:28])
```

```{r, include=FALSE}
# Fit models with uniform prior
uniform_hierarchical <- stan(file = "Uniform_hierarchical.stan", data = data_hierarchical)
uniform_separate <- stan(file = "Uniform_separate.stan", data = data_separate)

# Fit models with other prior combo
second_hierarchical <- stan(file = "2ndprior_hierarchical.stan", data = data_hierarchical)
second_separate <- stan(file = "2ndprior_separate.stan", data = data_separate)

hier_sens_df <- as.data.frame(colMeans(as.data.frame(extract(uniform_hierarchical, 'yrep_r'))))
rownames(hier_sens_df) <- NULL
colnames(hier_sens_df) <- c('value')
hier_sens_df['Prior'] <- 'Uniform'
hier_sens_df['x'] <- as.numeric(rownames(hier_sens_df))
temp_df <- as.data.frame(colMeans(as.data.frame(extract(second_hierarchical, 'yrep_r'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
hier_sens_df <- rbind(hier_sens_df, temp_df)
temp_df <- as.data.frame(colMeans(as.data.frame(extract(fit_hierarchical, 'yrep_r'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
hier_sens_df <- rbind(hier_sens_df, temp_df)
hier_sens_df['y_type'] <- 'Replicate'

temp_2 <- as.data.frame(extract(uniform_hierarchical, 'ypred_r'))
rownames(temp_2) <- NULL
colnames(temp_2) <- c('value')
temp_2['Prior'] <- 'Uniform'
temp_2['x'] <- as.numeric(rownames(temp_2))
temp_df <- as.data.frame(extract(second_hierarchical, 'ypred_r'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_df <- as.data.frame(extract(fit_hierarchical, 'ypred_r'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_2['y_type'] <- 'Prediction'
hier_sens_df <- rbind(hier_sens_df, temp_2)
hier_sens_df['col_val'] <- 'Recycled'

temp_3 <- as.data.frame(colMeans(as.data.frame(extract(uniform_hierarchical, 'yrep_c'))))
rownames(temp_3) <- NULL
colnames(temp_3) <- c('value')
temp_3['Prior'] <- 'Uniform'
temp_3['x'] <- as.numeric(rownames(temp_3))
temp_df <- as.data.frame(colMeans(as.data.frame(extract(second_hierarchical, 'yrep_c'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_df <- as.data.frame(colMeans(as.data.frame(extract(fit_hierarchical, 'yrep_c'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_3['y_type'] <- 'Replicate'

temp_2 <- as.data.frame(extract(uniform_hierarchical, 'ypred_c'))
rownames(temp_2) <- NULL
colnames(temp_2) <- c('value')
temp_2['Prior'] <- 'Uniform'
temp_2['x'] <- as.numeric(rownames(temp_2))
temp_df <- as.data.frame(extract(second_hierarchical, 'ypred_c'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_df <- as.data.frame(extract(fit_hierarchical, 'ypred_c'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_2['y_type'] <- 'Prediction'
temp_3 <- rbind(temp_3, temp_2)
temp_3['col_val'] <- 'Combusted'
hier_sens_df <- rbind(hier_sens_df, temp_3)

temp_3 <- as.data.frame(colMeans(as.data.frame(extract(uniform_hierarchical, 'yrep_l'))))
rownames(temp_3) <- NULL
colnames(temp_3) <- c('value')
temp_3['Prior'] <- 'Uniform'
temp_3['x'] <- as.numeric(rownames(temp_3))
temp_df <- as.data.frame(colMeans(as.data.frame(extract(second_hierarchical, 'yrep_l'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_df <- as.data.frame(colMeans(as.data.frame(extract(fit_hierarchical, 'yrep_l'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_3['y_type'] <- 'Replicate'

temp_2 <- as.data.frame(extract(uniform_hierarchical, 'ypred_l'))
rownames(temp_2) <- NULL
colnames(temp_2) <- c('value')
temp_2['Prior'] <- 'Uniform'
temp_2['x'] <- as.numeric(rownames(temp_2))
temp_df <- as.data.frame(extract(second_hierarchical, 'ypred_l'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_df <- as.data.frame(extract(fit_hierarchical, 'ypred_l'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_2['y_type'] <- 'Prediction'
temp_3 <- rbind(temp_3, temp_2)
temp_3['col_val'] <- 'Landfilled'
hier_sens_df <- rbind(hier_sens_df, temp_3)
```

```{r include=FALSE}
sep_sens_df <- as.data.frame(colMeans(as.data.frame(extract(uniform_separate, 'yrep_r'))))
rownames(sep_sens_df) <- NULL
colnames(sep_sens_df) <- c('value')
sep_sens_df['Prior'] <- 'Uniform'
sep_sens_df['x'] <- as.numeric(rownames(sep_sens_df))
temp_df <- as.data.frame(colMeans(as.data.frame(extract(second_separate, 'yrep_r'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
sep_sens_df <- rbind(sep_sens_df, temp_df)
temp_df <- as.data.frame(colMeans(as.data.frame(extract(fit_separate, 'yrep_r'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
sep_sens_df <- rbind(sep_sens_df, temp_df)
sep_sens_df['y_type'] <- 'Replicate'

temp_2 <- as.data.frame(extract(uniform_separate, 'ypred_r'))
rownames(temp_2) <- NULL
colnames(temp_2) <- c('value')
temp_2['Prior'] <- 'Uniform'
temp_2['x'] <- as.numeric(rownames(temp_2))
temp_df <- as.data.frame(extract(second_separate, 'ypred_r'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_df <- as.data.frame(extract(fit_separate, 'ypred_r'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_2['y_type'] <- 'Prediction'
sep_sens_df <- rbind(sep_sens_df, temp_2)
sep_sens_df['col_val'] <- 'Recycled'

temp_3 <- as.data.frame(colMeans(as.data.frame(extract(uniform_separate, 'yrep_c'))))
rownames(temp_3) <- NULL
colnames(temp_3) <- c('value')
temp_3['Prior'] <- 'Uniform'
temp_3['x'] <- as.numeric(rownames(temp_3))
temp_df <- as.data.frame(colMeans(as.data.frame(extract(second_separate, 'yrep_c'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_df <- as.data.frame(colMeans(as.data.frame(extract(fit_separate, 'yrep_c'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_3['y_type'] <- 'Replicate'

temp_2 <- as.data.frame(extract(uniform_separate, 'ypred_c'))
rownames(temp_2) <- NULL
colnames(temp_2) <- c('value')
temp_2['Prior'] <- 'Uniform'
temp_2['x'] <- as.numeric(rownames(temp_2))
temp_df <- as.data.frame(extract(second_separate, 'ypred_c'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_df <- as.data.frame(extract(fit_separate, 'ypred_c'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_2['y_type'] <- 'Prediction'
temp_3 <- rbind(temp_3, temp_2)
temp_3['col_val'] <- 'Combusted'
sep_sens_df <- rbind(sep_sens_df, temp_3)

temp_3 <- as.data.frame(colMeans(as.data.frame(extract(uniform_separate, 'yrep_l'))))
rownames(temp_3) <- NULL
colnames(temp_3) <- c('value')
temp_3['Prior'] <- 'Uniform'
temp_3['x'] <- as.numeric(rownames(temp_3))
temp_df <- as.data.frame(colMeans(as.data.frame(extract(second_separate, 'yrep_l'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_df <- as.data.frame(colMeans(as.data.frame(extract(fit_separate, 'yrep_l'))))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_3 <- rbind(temp_3, temp_df)
temp_3['y_type'] <- 'Replicate'

temp_2 <- as.data.frame(extract(uniform_separate, 'ypred_l'))
rownames(temp_2) <- NULL
colnames(temp_2) <- c('value')
temp_2['Prior'] <- 'Uniform'
temp_2['x'] <- as.numeric(rownames(temp_2))
temp_df <- as.data.frame(extract(second_separate, 'ypred_l'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Second'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_df <- as.data.frame(extract(fit_separate, 'ypred_l'))
rownames(temp_df) <- NULL
colnames(temp_df) <- c('value')
temp_df['Prior'] <- 'Original'
temp_df['x'] <- as.numeric(rownames(temp_df))
temp_2 <- rbind(temp_2, temp_df)
temp_2['y_type'] <- 'Prediction'
temp_3 <- rbind(temp_3, temp_2)
temp_3['col_val'] <- 'Landfilled'
sep_sens_df <- rbind(sep_sens_df, temp_3)
```

## Separate

Sensitivity analysis for the separate model can be seen in Figure \@ref(fig:sep-sens-plot).

```{r sep-sens-plot, fig.height=4, fig.width=7, fig.cap="Sensitivity analysis for separate model. Prediction refers to 2019 prediction, and Replicate refers to the original dataset replicated with the model."}
ggplot(data=sep_sens_df, aes(x=value, color = Prior)) +
  geom_density() +
  facet_wrap(y_type ~ col_val, scales='free') +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
```

From the figure, we can see that the separate model is quite sensitive to prior choice. Although there isn't much difference between the effect of the final priors and the uniform prior, the "Second" prior makes quite a large difference.

## Hierarchical

Sensitivity analysis for the hierarchical model can be seen in Figure \@ref(fig:hier-sens-plot).

```{r hier-sens-plot, fig.height=4, fig.width=7, fig.cap="Sensitivity analysis for hierarchical model. Prediction refers to 2019 prediction, and Replicate refers to the original dataset replicated with the model."}
ggplot(data=hier_sens_df, aes(x=value, color = Prior)) +
  geom_density() +
  facet_wrap(y_type ~ col_val, scales='free') +
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=0))
```

From the figure, we can see that the hierarchical model is much less sensitive to prior choice than the separate model. For the predicted quantities the prior choice doesn't seem to have a significant effect, although the "Second" prior clearly results in a slightly narrower distribution than the others. For the replicated quantities, the prior choice doesn't have much effect on the "Landfilled" block, but the model seems more sensitive in the other blocks where the uniform prior is clearly causing some differences.

# Model comparison (LOO-CV)

The k-hat values are visualised in Figure \@ref(fig:separate-k-plot) for the separate model, and in Figure \@ref(fig:hierarchical-k-plot) for the hierarchical model. All k-values for both models are below 0.7, meaning the PSIS-LOO values for each model are reliable.

```{r separate-k-plot, fig.height=4, fig.width=5, fig.cap="k-hat values for the separate model.", echo=FALSE, warning=FALSE}
#PSIS-LOO for separate model
sep_loo <- rstan::loo(fit_separate)
sep_elpd <- sep_loo$estimates[1,1]
# Effective number of parameters:
sep_p <- sep_loo$estimates[2,1]
plot(sep_loo)
```

```{r hierarchical-k-plot, fig.height=4, fig.width=5, fig.cap="k-hat values for the hierarchical model.", echo=FALSE, warning=FALSE}
#PSIS-LOO for hierarchical model
hier_loo <- rstan::loo(fit_hierarchical)
hier_elpd <- hier_loo$estimates[1,1]
# Effective number of parameters:
hier_p <- hier_loo$estimates[2,1]
plot(hier_loo)

```

The elpd values and effective number of parameters (p_eff) can be seen in Table \@ref(tab:loo-tab) for both the hierarchical and separate models.

```{r loo-tab, echo=FALSE, results='asis'}
loo_table <- rbind(c(hier_elpd, sep_elpd), c(hier_p, sep_p))
colnames(loo_table) <- c('Hierarchical', 'Separate')
rownames(loo_table) <- c('elpd', 'p_eff')
kable(loo_table, caption='LOO-CV results')

```

We can see that the separate model has a slightly larger elpd-value, hence the separate model would be selected according to PSIS-LOO.

# Discussion

One area where this project could be improved is plotting. There were some issues with the formatting of the report and the scaling of the plots that could not be remedied in time.

Another aspect to be improved on would be the selection and cleaning of the dataset. Our chosen dataset has irregular time periods of data collection, and we had to account for this when designing our models. It would be preferable to have more data, that is grouped annually. 

The fit of the model is made worse because of the small dataset, but we used our priors to try and mitigate that factor when ftting our models.

# Conclusion

Through the course of this project, we have gone through the process of fitting our [dataset](#the-final-data) to two Stan models, a hierarchical model and a separate model, both of which fit linear Gaussian models to the features. We compare the R-hat and ESS diagnostics of both models to see if they converge or not (they do.) We then use the HMC diagnostics to verify if random samples drawn from the posterior distribution converge.

We used these metrics in the early stages of the project to troubleshoot model design and changed parts of our model to optimize for them. A [predictive performance assessment](#predictive-performance-assessment) was conducted to test if the model produced good predictions, where we predicted the data of year 2018, and compared it with the real values.

Our final finding is that the separate model fit the data better.

We gained some insights in working with smaller datasets, and have observed that the smaller the dataset, the more important the priors are in building a good model. This highlights the importance of domain expertise, and has applications in fields with small datasets like the measurements of particle accelerators or the treatment outcomes of patients with rare cancers.

In a future project, we could spend more time researching the topic and building up domain expertise to generate more informative priors, which have a very large impact on the performances of our models. We could also fit more models to our data and perform more in-depth cross validation of the models. 

# Self reflection

We learned how to combine everything learned in the assignments into the evaluation of chosen models with a real-world dataset. We also learned how to adapt various model evaluation methods to a model with several categories. There were some challenges like unclean data and coordinating between group members, but we learned a lot along the way, and are satisfied with the results of our labours.

# References
